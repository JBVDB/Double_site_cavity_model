{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7NDjJd2Frpm"
   },
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bsKq3JlQtrR"
   },
   "source": [
    "## Import packages, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to install for double cav model:\n",
    "\n",
    "- seaborn 0.11.1\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iRwuaLYBMZ6"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import timeit\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import Bio.SeqIO\n",
    "import Bio.PDB\n",
    "\n",
    "# Packages for parsing and investigate PDB files\n",
    "# import pdbfixer\n",
    "# import simtk\n",
    "# import simtk.openmm\n",
    "# import simtk.openmm.app\n",
    "# import simtk.unit\n",
    "\n",
    "# Utility packages\n",
    "from getpass import getpass\n",
    "from IPython.display import Audio\n",
    "import smtplib\n",
    "import ssl\n",
    "import tqdm.notebook as tqdm\n",
    "import traceback\n",
    "import inspect\n",
    "\n",
    "# os.chdir(\"gdrive/My Drive/thesis/\")\n",
    "\n",
    "# For consistency:\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import tested_double_cav_models as test\n",
    "\n",
    "# Personal modules\n",
    "# from neptune.cavity_model import (\n",
    "#     CavityModel,\n",
    "#     ResidueEnvironment,\n",
    "#     ResidueEnvironmentsDataset,\n",
    "#     DownstreamModel,\n",
    "#     ToTensor,\n",
    "#     DDGDataset,\n",
    "#     DDGToTensor,\n",
    "#     DownstreamModel,\n",
    "# )\n",
    "\n",
    "# from neptune.helpers import (\n",
    "#     _augment_with_reverse_mutation,\n",
    "#     _populate_dfs_with_nlls_and_nlfs,\n",
    "#     _populate_dfs_with_resenvs,\n",
    "#     _train_loop,\n",
    "#     _train_val_split,\n",
    "#     _get_ddg_training_dataloaders,\n",
    "#     _get_ddg_validation_dataloaders,\n",
    "#     _train_downstream_and_evaluate,\n",
    "#     _predict_with_downstream,\n",
    "#     _eval_loop,\n",
    "#     _test_cavity_model,\n",
    "# )\n",
    "\n",
    "# from visualization import (\n",
    "#     plot_training_history,\n",
    "#     plot_training_history_v2,\n",
    "#     show_respair_acc_heatmap,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbaMCoaV0jB7"
   },
   "outputs": [],
   "source": [
    "from visualization import (\n",
    "    plot_training_history,\n",
    "    plot_training_history_v2,\n",
    "    show_respair_acc_heatmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAQzu0InzxZT"
   },
   "outputs": [],
   "source": [
    "from log_to_neptune import (\n",
    "    set_and_save_metadata,\n",
    "    log_metadata_to_neptune,\n",
    "    convert_plt_to_plotly\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YHMqnHyxEFa"
   },
   "outputs": [],
   "source": [
    "# !pip install chardet==3.0.4\n",
    "# !pip install rmsd\n",
    "# !pip install openpyxl\n",
    "# !pip install numpy==1.16\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9TnNbnBvoj9"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/gao666999/SSBONDPredict.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eo5MZppZv9q6"
   },
   "outputs": [],
   "source": [
    "import Bio\n",
    "import Bio.PDB\n",
    "import Bio.PDB.vectors\n",
    "import numpy as np\n",
    "import simtk\n",
    "import simtk.openmm\n",
    "import simtk.openmm.app\n",
    "import simtk.unit\n",
    "\n",
    "# from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwrPqsHK-sEM"
   },
   "source": [
    "## cavity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LwlOd0y-u5j"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "# public objects of that module that will be exported when from <module> import * is used on the module (overrides default _objects)\n",
    "\n",
    "\n",
    "class ResidueEnvironment:\n",
    "    \"\"\"\n",
    "    Residue environment class used to hold necessarry information about the\n",
    "    atoms of the environment such as atomic coordinates, atom types and the\n",
    "    class of the missing (non-central) TWO amino acids\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz_coords: np.ndarray\n",
    "        Numpy array with shape (n_atoms, 3) containing the x, y, z coordinates.\n",
    "    atom_types: np.ndarray\n",
    "        1D numpy array containing the atom types. Integer values in range(6).\n",
    "    restypes_onehot: np.ndarray # -> TO DOUBLE\n",
    "        Numpy array with shape (n_atoms, 21) containing the amino acid\n",
    "        class of the missing amino acid\n",
    "    chain_id: str\n",
    "        Chain id associated to ResidueEnvironment object\n",
    "    pdb_residue_number: int # -> TO DOUBLE\n",
    "        Residue number associated with the ResidueEnvironment object\n",
    "    pdb_id: str\n",
    "        PDBID associated with the ResidueEnvironment object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xyz_coords: np.ndarray,\n",
    "        atom_types: np.ndarray,\n",
    "        restype_onehot: np.ndarray,\n",
    "        chain_id: str,\n",
    "        pdb_residue_number: int,\n",
    "        pdb_id: str,\n",
    "    ):\n",
    "        self._xyz_coords = xyz_coords\n",
    "        self._atom_types = atom_types\n",
    "        self._restype_onehot = restype_onehot # -> TO DOUBLE\n",
    "        self._chain_id = chain_id\n",
    "        self._pdb_residue_number = pdb_residue_number # -> TO DOUBLE\n",
    "        self._pdb_id = pdb_id\n",
    "        \n",
    "    @property\n",
    "    def xyz_coords(self):\n",
    "        return self._xyz_coords\n",
    "\n",
    "    @property\n",
    "    def atom_types(self):\n",
    "        return self._atom_types\n",
    "\n",
    "    @property\n",
    "    def restype_onehot(self):\n",
    "        return self._restype_onehot\n",
    "\n",
    "    @property\n",
    "    def restype_index(self):\n",
    "        return np.argmax(self.restype_onehot)\n",
    "    \n",
    "    @property\n",
    "    def chain_id(self):\n",
    "        return self._chain_id\n",
    "\n",
    "    @property\n",
    "    def pdb_residue_number(self):\n",
    "        return self._pdb_residue_number\n",
    "\n",
    "    @property\n",
    "    def pdb_id(self):\n",
    "        return self._pdb_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Used to represent a classÃ¢â‚¬â„¢s objects as a string.\n",
    "        Built-in fct for calling it: repr()\n",
    "        \"\"\"\n",
    "        return (\n",
    "            f\"<ResidueEnvironment with {self.xyz_coords.shape[0]} atoms. \" # it calls property self.xyz_coords\n",
    "            f\"pdb_id: {self.pdb_id}, \"\n",
    "            f\"chain_id: {self.chain_id}, \"\n",
    "            f\"pdb_residue_number: {self.pdb_residue_number}, \" \n",
    "            f\"restype_index: {self.restype_index}>\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ResidueEnvironmentsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Residue environment dataset class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data: Union[List[str], List[ResidueEnvironment]]\n",
    "        List of parsed pdb filenames in .npz format or list of\n",
    "        ResidueEnvironment objects\n",
    "    transform: Callable\n",
    "        A to-tensor transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_data: Union[List[str], List[ResidueEnvironment]], # Union[X, Y] means either X or Y\n",
    "        transformer: Callable = None,\n",
    "    ):\n",
    "        if all(isinstance(x, ResidueEnvironment) for x in input_data):\n",
    "            self._res_env_objects = input_data\n",
    "        elif all(isinstance(x, str) for x in input_data):\n",
    "            self._res_env_objects = self._parse_envs(input_data)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Input data is not of type\" \"Union[List[str], List[ResidueEnvironment]]\"\n",
    "            )\n",
    "\n",
    "        self._transformer = transformer\n",
    "\n",
    "    @property\n",
    "    def res_env_objects(self):\n",
    "        return self._res_env_objects\n",
    "\n",
    "    @property\n",
    "    def transformer(self):\n",
    "        return self._transformer\n",
    "\n",
    "    @transformer.setter\n",
    "    def transformer(self, transformer):\n",
    "        \"\"\"TODO: Think if a constraint to add later\"\"\"\n",
    "        self._transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.res_env_objects)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.res_env_objects[idx]\n",
    "        if self.transformer:\n",
    "            sample = self.transformer(sample)\n",
    "        return sample\n",
    "\n",
    "    def _parse_envs(self, npz_filenames: List[str]) -> List[ResidueEnvironment]:\n",
    "        \"\"\"\n",
    "        TODO: Make this more readable\n",
    "        \"\"\"\n",
    "        res_env_objects = []\n",
    "        for i in tqdm.tnrange(len(npz_filenames)):\n",
    "            coordinate_features = np.load(npz_filenames[i])\n",
    "            atom_coords_prot_seq = coordinate_features[\"positions\"] # atom coords\n",
    "            restypes_onehots_prot_seq = coordinate_features[\"pair_aa_onehot\"]\n",
    "            selector_prot_seq = coordinate_features[\"selector\"] # atom ids\n",
    "            atom_types_flattened = coordinate_features[\"atom_types_numeric\"]\n",
    "\n",
    "            chain_ids = coordinate_features[\"chain_ids\"]\n",
    "            pdb_residue_numbers = coordinate_features[\"pair_res_indices\"]\n",
    "            chain_boundary_indices = coordinate_features[\"chain_boundary_indices\"]\n",
    "\n",
    "            pdb_id = os.path.basename(npz_filenames[i])[0:4]\n",
    "\n",
    "            N_pair_residues = selector_prot_seq.shape[0] # WILL BECOME N_PAIRS!!\n",
    "\n",
    "            for pair_res_i in range(N_pair_residues):\n",
    "                # Get atom indexes\n",
    "                selector = selector_prot_seq[pair_res_i]\n",
    "                selector_masked = selector[selector > -1]  # Remove Filler -1\n",
    "                \n",
    "                # Get atom types\n",
    "                atom_types = atom_types_flattened[selector_masked]\n",
    "                \n",
    "                # Get atom coordinates\n",
    "                coords_mask = (\n",
    "                    atom_coords_prot_seq[pair_res_i, :, 0] != -99.0 # for all its atoms, only need to check one column of coord for it (x here)\n",
    "                )  # Remove filler\n",
    "                coords = atom_coords_prot_seq[pair_res_i][coords_mask]\n",
    "                \n",
    "                # Get resi_evt ONE-HOT label (Target variable) -> TO DOUBLE\n",
    "                restype_onehot = restypes_onehots_prot_seq[pair_res_i]\n",
    "                \n",
    "                # Get resi real id -> TO DOUBLE\n",
    "                pdb_residue_number = pdb_residue_numbers[pair_res_i]\n",
    "                \n",
    "                # Locate chain id -> TO DOUBLE\n",
    "                for j in range(len(chain_ids)):\n",
    "                    chain_boundary_0 = chain_boundary_indices[j]\n",
    "                    chain_boundary_1 = chain_boundary_indices[j + 1]\n",
    "                    if pair_res_i in range(chain_boundary_0, chain_boundary_1):\n",
    "                        chain_id = str(chain_ids[j])\n",
    "                        break\n",
    "\n",
    "                res_env_objects.append(\n",
    "                    ResidueEnvironment(\n",
    "                        coords,\n",
    "                        atom_types,\n",
    "                        restype_onehot, # -> TO DOUBLE\n",
    "                        chain_id, # -> TO DOUBLE\n",
    "                        pdb_residue_number, # -> TO DOUBLE\n",
    "                        pdb_id,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return res_env_objects\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"\n",
    "    To-tensor transformer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device: str\n",
    "        Either \"cuda\" (gpu) or \"cpu\". Is set-able.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        device: str,\n",
    "        unravel_index=True,\n",
    "        reshape_index=True,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.unravel_index = unravel_index\n",
    "        self.reshape_index = reshape_index\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.__device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device):\n",
    "        allowed_devices = [\"cuda\", \"cpu\"]\n",
    "        if device in allowed_devices:\n",
    "            self.__device = device\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'chosen device \"{device}\" not in {allowed_devices}.')\n",
    "\n",
    "    def __call__(self, sample: ResidueEnvironment,):\n",
    "        \"\"\"Converts single ResidueEnvironment object into x_ and y_\"\"\"\n",
    "\n",
    "        sample_env = np.hstack(\n",
    "            [np.reshape(sample.atom_types, [-1, 1]), sample.xyz_coords]\n",
    "        )\n",
    "        if self.reshape_index:\n",
    "            return {\n",
    "                \"x_\": torch.tensor(sample_env, dtype=torch.float32\n",
    "                    ).to(self.device),\n",
    "                \"y_\": self.reshape_pairres_indices(sample.restype_onehot,\n",
    "                                                   n_aa_in=20,\n",
    "                    ).to(self.device),\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            return {\n",
    "                \"x_\": torch.tensor(sample_env, dtype=torch.float32\n",
    "                    ).to(self.device),\n",
    "                \"y_\": torch.tensor(sample.restype_onehot, dtype=torch.int8\n",
    "                    ).to(self.device),\n",
    "            }\n",
    "\n",
    "\n",
    "    def reshape_pairres_indices(self, targets: np.array, n_aa_in=20, n_aa_out=20):\n",
    "        \"\"\"\n",
    "        Convert pair_res onehot encoding to individual res encoding.\n",
    "        array((n_pairs, n_aa_in*n_aa_in)) -> tensor((n_pairs, 2, n_aa_out*2))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        indices = np.unravel_index(np.argmax(targets), shape=(n_aa_in,\n",
    "                                                             n_aa_in))\n",
    "        if self.unravel_index:\n",
    "            one_hot_arr = torch.zeros((2, n_aa_out), dtype=torch.int8)\n",
    "            one_hot_arr[0, indices[0]] = 1\n",
    "            one_hot_arr[1, indices[1]] = 1\n",
    "        else:\n",
    "            one_hot_arr = torch.zeros((n_aa_out*n_aa_out), dtype=torch.int8)\n",
    "            indices = np.ravel_multi_index(np.vstack(indices),\n",
    "                                           dims=(n_aa_out, n_aa_out)\n",
    "                                           )\n",
    "            one_hot_arr[indices] = 1\n",
    "\n",
    "        return one_hot_arr\n",
    "\n",
    "    def collate_cat(self, batch: List[ResidueEnvironment]):\n",
    "        \"\"\"\n",
    "        Collate method used by the dataloader to collate a\n",
    "        batch of ResidueEnvironment objects.\n",
    "        \"\"\"\n",
    "        target = torch.cat([torch.unsqueeze(b[\"y_\"], 0) for b in batch], dim=0)\n",
    "\n",
    "        # To collate the input, we need to add a column which\n",
    "        # specifies the environment each atom belongs to = its evt (in the radius zone or the res)!!! So we add an evt \"pseudo_id in the batch\"\n",
    "        env_id_batch = []\n",
    "        for i, b in enumerate(batch): # b is one protein in the batch\n",
    "            n_atoms = b[\"x_\"].shape[0]\n",
    "            env_id_arr = torch.zeros(n_atoms, dtype=torch.float32).to(self.device) + i # i is this pseudo_id, to device to be in the same device ax x\n",
    "            env_id_batch.append(\n",
    "                torch.cat([torch.unsqueeze(env_id_arr, 1), b[\"x_\"]], dim=1) # add one column\n",
    "            )\n",
    "        data = torch.cat(env_id_batch, dim=0) # stack all the proteins'atoms on x axis\n",
    "\n",
    "        return data, target\n",
    "\n",
    "\n",
    "class CavityModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    3D convolutional neural network to missing amino acid classification\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device: str\n",
    "        Either \"cuda\" (gpu) or \"cpu\". Is set-able.\n",
    "    n_atom_types: int\n",
    "        Number of atom types. (C, H, N, O, S, P)\n",
    "    bins_per_angstrom: float\n",
    "        Number of grid points per Angstrom.\n",
    "    grid_dim: int\n",
    "        Grid dimension\n",
    "    sigma: float\n",
    "        Standard deviation used for gaussian blurring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: str,\n",
    "        n_atom_types: int = 6,\n",
    "        bins_per_angstrom: float = 1.0,\n",
    "        grid_dim_xy: int = 8, # because 9 Angstrom of radius\n",
    "        grid_dim_z: int = 16,\n",
    "        sigma: float = 0.6,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self._n_atom_types = n_atom_types\n",
    "        self._bins_per_angstrom = bins_per_angstrom\n",
    "        self._grid_dim_xy = grid_dim_xy\n",
    "        self._grid_dim_z = grid_dim_z\n",
    "        self._sigma = sigma\n",
    "\n",
    "        self._model()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.__device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device):\n",
    "        allowed_devices = [\"cuda\", \"cpu\"]\n",
    "        if device in allowed_devices:\n",
    "            self.__device = device\n",
    "        else:\n",
    "            raise ValueError('chosen device \"{device}\" not in {allowed_devices}')\n",
    "\n",
    "    @property\n",
    "    def n_atom_types(self):\n",
    "        return self._n_atom_types\n",
    "\n",
    "    @property\n",
    "    def bins_per_angstrom(self):\n",
    "        return self._bins_per_angstrom\n",
    "\n",
    "    @property\n",
    "    def grid_dim_xy(self):\n",
    "        return self._grid_dim_xy\n",
    "\n",
    "    @property\n",
    "    def grid_dim_z(self):\n",
    "        return self._grid_dim_z\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return self._sigma\n",
    "\n",
    "    @property\n",
    "    def sigma_p(self):\n",
    "        return self.sigma * self.bins_per_angstrom\n",
    "\n",
    "    @property\n",
    "    def lin_spacing_xy(self):\n",
    "        lin_spacing_xy = np.linspace(\n",
    "            start=-self.grid_dim_xy / 2 * self.bins_per_angstrom \n",
    "            + self.bins_per_angstrom / 2,\n",
    "            stop=self.grid_dim_xy / 2 * self.bins_per_angstrom\n",
    "            - self.bins_per_angstrom / 2,\n",
    "            num=self.grid_dim_xy,\n",
    "        )\n",
    "        return lin_spacing_xy\n",
    "\n",
    "    @property\n",
    "    def lin_spacing_z(self):\n",
    "        lin_spacing_z = np.linspace(\n",
    "            start=-self.grid_dim_z / 2 * self.bins_per_angstrom \n",
    "            + self.bins_per_angstrom / 2,\n",
    "            stop=self.grid_dim_z / 2 * self.bins_per_angstrom\n",
    "            - self.bins_per_angstrom / 2,\n",
    "            num=self.grid_dim_z,\n",
    "        )\n",
    "        return lin_spacing_z\n",
    "\n",
    "    def _model(self):\n",
    "        self.xx, self.yy, self.zz = torch.tensor(\n",
    "            np.meshgrid(\n",
    "                self.lin_spacing_xy, self.lin_spacing_xy, self.lin_spacing_z, indexing=\"ij\" # matrix indexing (classic python)\n",
    "            ),\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device) # normally, already on \"cuda\"\n",
    "\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(6, 16, kernel_size=(3, 3, 3), padding=1), # output = [100, 16, 4, 4 ,8]\n",
    "            torch.nn.MaxPool3d(kernel_size=2),\n",
    "            torch.nn.BatchNorm3d(16),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1), # usual: padding = round(kernel_size/2, lower), output = [100, 32, 2, 2, 4]\n",
    "            torch.nn.MaxPool3d(kernel_size=2),\n",
    "            torch.nn.BatchNorm3d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1), # output = [100, 128, 1, 1, 2]\n",
    "            torch.nn.MaxPool3d(kernel_size=2),\n",
    "            torch.nn.BatchNorm3d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "        self.dense1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=128, out_features=256), # bachnorm filters 64 * 4 parameters of batch norm per filter\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.dense2 = torch.nn.Linear(in_features=256, out_features=40)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._gaussian_blurring(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x) # yields logits, mapped to probabilities afterwards by the Softmax fct.\n",
    "        return x\n",
    "\n",
    "    def _gaussian_blurring(self, x: torch.Tensor) -> torch.Tensor: # increase the resolution of the signal, reduce noises by blurring/smoothing intensity transitions of densities for each channel of atom type.\n",
    "        \"\"\"\n",
    "        Method that takes 2d torch.Tensor describing the atoms of the batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Tensor for shape (n_atoms, 5). Each row represents an atom, where:\n",
    "                column 0 describes the environment of the batch the\n",
    "                atom belongs to\n",
    "                column 1 describes the atom type\n",
    "                column 2,3,4 are the x, y, z coordinates, respectively\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fields_torch: torch.Tensor\n",
    "            Represents the structural environment (density val in 3d meshgrid)\n",
    "            with gaussian blurring and has shape (-1, 6, self.grid_dim_xy,\n",
    "                                                         self.grid_dim_xy,\n",
    "                                                         self.grid_dim_z).\n",
    "        \"\"\"\n",
    "        current_batch_size = torch.unique(x[:, 0]).shape[0]\n",
    "        fields_torch = torch.zeros(\n",
    "            (\n",
    "                current_batch_size,\n",
    "                self.n_atom_types,\n",
    "                self.grid_dim_xy,\n",
    "                self.grid_dim_xy,\n",
    "                self.grid_dim_z,\n",
    "            )\n",
    "        ).to(self.device)\n",
    "        for j in range(self.n_atom_types): # per batch\n",
    "            mask_j = x[:, 1] == j\n",
    "            atom_type_j_data = x[mask_j] # select all atoms of that type\n",
    "            if atom_type_j_data.shape[0] > 0:\n",
    "            # Fancy broadcasting:\n",
    "            # reshaped_.xx.shape = (8*8*16, 1) : flattened x coordinates\n",
    "            # pos[:, 0].shape = (n_atom_j, 1)\n",
    "            # -> (reshaped_xx - pos[:, 0]).shape = (8*8*16, n_atom_j) : flattened density values, x axis contribution\n",
    "                pos = atom_type_j_data[:, 2:]\n",
    "                density = torch.exp(\n",
    "                    -(\n",
    "                        (torch.reshape(self.xx, [-1, 1]) - pos[:, 0]) ** 2\n",
    "                        + (torch.reshape(self.yy, [-1, 1]) - pos[:, 1]) ** 2\n",
    "                        + (torch.reshape(self.zz, [-1, 1]) - pos[:, 2]) ** 2\n",
    "                    )\n",
    "                    / (2 * self.sigma_p ** 2)\n",
    "                )\n",
    "\n",
    "                # Normalize each atom density to 1 (over whole batch), atom being x axis (dim=0)\n",
    "                density /= torch.sum(density, dim=0)\n",
    "\n",
    "                # Since column 0 of atom_type_j_data is SORTED\n",
    "                # I can use a trick to detect the boundaries of environment based\n",
    "                # on the change from one value to another.\n",
    "                change_mask_j = (\n",
    "                    atom_type_j_data[:, 0][:-1] != atom_type_j_data[:, 0][1:] # when !=, that means the previous and next indexes are the limits\n",
    "                )\n",
    "\n",
    "                # Add begin and end indices\n",
    "                ranges_i = torch.cat(\n",
    "                    [\n",
    "                        torch.tensor([0]), # we start from 0\n",
    "                        torch.arange(atom_type_j_data.shape[0] - 1)[change_mask_j] + 1,\n",
    "                        torch.tensor([atom_type_j_data.shape[0]]), # we must end with the last environment for sure\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Fill tensor, for each residual environment (i) of the batch\n",
    "                for i in range(ranges_i.shape[0]):\n",
    "                    if i < ranges_i.shape[0] - 1:\n",
    "                        index_0, index_1 = ranges_i[i], ranges_i[i + 1]\n",
    "                        fields = torch.reshape(\n",
    "                            torch.sum(density[:, index_0:index_1], dim=1), # densities of the res_evt voxel\n",
    "                            [self.grid_dim_xy, self.grid_dim_xy, self.grid_dim_z], # get back rectangular cuboid shape\n",
    "                        )\n",
    "                        fields_torch[i, j, :, :, :] = fields # density for that voxel\n",
    "        return fields_torch\n",
    "\n",
    "\n",
    "class DownstreamModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Downstream FC neural network with 1 hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Model\n",
    "        self.lin1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(44, 10),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.lin2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 10),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.lin3 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DDGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ddG dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        transformer: Callable = None,\n",
    "    ):\n",
    "\n",
    "        self._df = df\n",
    "        self.transformer = transformer\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def transformer(self):\n",
    "        return self._transformer\n",
    "\n",
    "    @transformer.setter\n",
    "    def transformer(self, transformer):\n",
    "        \"\"\"TODO: Think if a constraint to add later\"\"\"\n",
    "        self._transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        if self.transformer:\n",
    "            sample = self.transformer(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class DDGToTensor:\n",
    "    \"\"\"\n",
    "    To-tensor transformer for ddG dataframe data\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample: pd.Series):\n",
    "        wt_onehot = np.zeros(20)\n",
    "        wt_onehot[sample[\"wt_idx\"]] = 1.0\n",
    "        mt_onehot = np.zeros(20)\n",
    "        mt_onehot[sample[\"mt_idx\"]] = 1.0\n",
    "\n",
    "        x_ = torch.cat(\n",
    "            [\n",
    "                torch.Tensor(wt_onehot),\n",
    "                torch.Tensor(mt_onehot),\n",
    "                torch.Tensor(\n",
    "                    [\n",
    "                        sample[\"wt_nll\"],\n",
    "                        sample[\"mt_nll\"],\n",
    "                        sample[\"wt_nlf\"],\n",
    "                        sample[\"mt_nlf\"],\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\"x_\": x_, \"y_\": sample[\"ddg\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dh60SjH99zh-"
   },
   "source": [
    "## matfact helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8UQfunyoYNY"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import timeit\n",
    "import pickle\n",
    "import glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# from cavity_model import (\n",
    "#     CavityModel,\n",
    "#     ResidueEnvironmentsDataset,\n",
    "#     ToTensor,\n",
    "#     DDGDataset,\n",
    "#     DDGToTensor,\n",
    "#     DownstreamModel,\n",
    "# )\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dataloader_train: DataLoader,\n",
    "    dataloader_val: DataLoader,\n",
    "    cavity_model_net: CavityModel,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    EPOCHS: int,\n",
    "    PATIENCE_CUTOFF: int,\n",
    "    matfact_k: int,\n",
    "    output_shape: int,\n",
    "    folder=\"models/double_cav_models\",\n",
    "    model_name=\"model\",\n",
    "    resume=False,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Helper function to perform training loop for the Cavity Model.\n",
    "    \"\"\"\n",
    "\n",
    "    current_best_epoch = -1\n",
    "    curr_best_metric = 1e4\n",
    "    patience = 0 # we start at 0\n",
    "    current_epoch = -1\n",
    "\n",
    "    early_stop_metric = \"loss\"\n",
    "    if loss_function.weight is not None: # no class imbalance correction\n",
    "        early_stop_metric = \"acc_join\"\n",
    "        curr_best_metric = -1\n",
    "\n",
    "    # Resume training\n",
    "    if len(glob.glob(f\"{folder}/{model_name}_epoch_*.pt\")) > 0:\n",
    "        epochs_saved = [int(x.split(\"_epoch_\")[1][:-3]) for x in glob.glob(\n",
    "            f\"{folder}/{model_name}*\")]\n",
    "\n",
    "        if resume:\n",
    "            current_epoch = max(epochs_saved)\n",
    "            model_path = f\"{folder}/{model_name}_epoch_{current_epoch}.pt\"\n",
    "            checkpoint = torch.load(model_path)\n",
    "\n",
    "            assert checkpoint[\"epoch\"] == current_epoch, (\n",
    "            f\"checkpoint epoch {checkpoint['epoch']}\",\n",
    "            f\"does not match current epoch {current_epoch}\"\n",
    "            )\n",
    "\n",
    "            print(f\"Training resumed from epoch {current_epoch}.\\n\")\n",
    "\n",
    "            current_best_epoch = checkpoint[\"current_best_epoch\"]\n",
    "            curr_best_metric = checkpoint[f\"current_best_{early_stop_metric}\"]\n",
    "            patience = checkpoint[\"patience\"]\n",
    "\n",
    "            cavity_model_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            print(f\"Current best epoch: {current_best_epoch}, \"\n",
    "                  f\"{early_stop_metric}: {curr_best_metric:5.3f}, \"\n",
    "                  f\"Patience: {patience}.\")\n",
    "            print()\n",
    "        else:\n",
    "            raise ValueError(f\"Epoch file(s) already exist(s) for {model_name}!\")\n",
    "\n",
    "    print(\n",
    "        f\"- Starts training with '{early_stop_metric}' \"\n",
    "        f\"as early stop metric...\\n\")\n",
    "    \n",
    "    # Create dict of rec to save.\n",
    "    rec = dict()\n",
    "\n",
    "    # Run model.\n",
    "    for epoch in range(current_epoch+1, EPOCHS+1): # EPOCHS+1 since 0 == ini state\n",
    "        t1 = timeit.default_timer()\n",
    "\n",
    "        # Assess model's initial state.\n",
    "        if epoch == 0:\n",
    "            print(\"Evaluating randomly initialized model.\")\n",
    "            loss_train, rec[\"train\"] =  _eval_loop(cavity_model_net,\n",
    "                                                dataloader_train,\n",
    "                                                loss_function,\n",
    "                                                matfact_k,\n",
    "                                                output_shape,\n",
    "                                                )\n",
    "        # Train over train batches.\n",
    "        else:\n",
    "            loss_train, rec[\"train\"] = _train_loop(cavity_model_net,\n",
    "                                                dataloader_train,\n",
    "                                                optimizer,\n",
    "                                                loss_function,\n",
    "                                                matfact_k,\n",
    "                                                output_shape,\n",
    "                                                )\n",
    "\n",
    "        rec[\"train\"][\"loss\"] = loss_train\n",
    "\n",
    "        print(f\"{'train'.upper():5s} - Loss: {rec['train']['loss']:5.3f}, \"\n",
    "        f\"Join acc: {rec['train']['acc_join']:5.3f},  \"\n",
    "        f\"Res1: {rec['train']['acc_res1']:4.2f}, \"\n",
    "        f\"Res2: {rec['train']['acc_res2']:4.2f},  \"\n",
    "        f\"Cond 2|1: {rec['train']['acc_res2_given_res1']:4.2f}, \"\n",
    "        f\"Cond 1|2: {rec['train']['acc_res1_given_res2']:4.2f}.\"\n",
    "        )\n",
    "\n",
    "        # Validate over val batches.\n",
    "        loss_val, rec[\"val\"] = _eval_loop(cavity_model_net,\n",
    "                                          dataloader_val,\n",
    "                                          loss_function,\n",
    "                                          matfact_k,\n",
    "                                          output_shape,\n",
    "                                          )\n",
    "        rec[\"val\"][\"loss\"] = loss_val\n",
    "\n",
    "        # Show epoch result\n",
    "        print(f\"{'val'.upper():5s} - Loss: {rec['val']['loss']:5.3f}, \"\n",
    "        f\"Join acc: {rec['val']['acc_join']:5.3f},  \"\n",
    "        f\"Res1: {rec['val']['acc_res1']:4.2f}, \"\n",
    "        f\"Res2: {rec['val']['acc_res2']:4.2f},  \"\n",
    "        f\"Cond 2|1: {rec['val']['acc_res2_given_res1']:4.2f}, \"\n",
    "        f\"Cond 1|2: {rec['val']['acc_res1_given_res2']:4.2f}.\"\n",
    "        )\n",
    "\n",
    "        if early_stop_metric == \"acc_join\":\n",
    "            if round(\n",
    "                rec[\"val\"][early_stop_metric] - curr_best_metric, 5) > 0.001:\n",
    "                curr_best_metric = rec[\"val\"][early_stop_metric]\n",
    "                current_best_epoch = epoch\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "        else:\n",
    "            if (\n",
    "                round(\n",
    "                curr_best_metric - rec[\"val\"][early_stop_metric], 5) > 0.001 or\n",
    "                epoch == 0):\n",
    "\n",
    "                curr_best_metric = rec[\"val\"][early_stop_metric]\n",
    "                current_best_epoch = epoch\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:2d} done in {round(timeit.default_timer() - t1, 2)} \"\n",
    "            f\"sec.  Patience: {patience}\")\n",
    "        print()\n",
    "\n",
    "        # Save training states for future resuming.\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": cavity_model_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"patience\": patience,\n",
    "            f\"current_best_{early_stop_metric}\": curr_best_metric,\n",
    "            \"current_best_epoch\": current_best_epoch,\n",
    "        }\n",
    "        model_path = f\"{folder}/{model_name}_epoch_{epoch}.pt\"\n",
    "\n",
    "        torch.save(state, model_path)\n",
    "\n",
    "        # Keep track rec (SAME ORDER as names_rec_to_save)\n",
    "        rec_path = f\"{folder}/metrics_{model_name}\"\n",
    "\n",
    "        if epoch > 0:\n",
    "            with open(f\"{rec_path}.pickle\", \"rb\") as handle:\n",
    "                history = pickle.load(handle)\n",
    "            for key in rec:\n",
    "                for metric in rec[key]:\n",
    "                    history[key][metric].append(rec[key][metric])\n",
    "            history[\"best_epoch\"] = current_best_epoch\n",
    "        else:\n",
    "            history = dict()\n",
    "            for key in rec:\n",
    "                history[key] = dict()\n",
    "                for metric in rec[key]:\n",
    "                    history[key][metric] = [rec[key][metric]]\n",
    "\n",
    "        # Pickle rec + best model name.        \n",
    "        with open(f\"{rec_path}.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(history, handle)\n",
    "\n",
    "        # Assess Early stopping.\n",
    "        if patience > PATIENCE_CUTOFF:\n",
    "            print(\"Early stopping activated.\")\n",
    "            break\n",
    "\n",
    "    best_model_path = f\"{folder}/{model_name}_epoch_{current_best_epoch}.pt\"\n",
    "    print(\n",
    "        f\"Best epoch idx: {current_best_epoch} with validation {early_stop_metric}: \"\n",
    "        f\"{curr_best_metric:5.3f}\\nFound at: \"\n",
    "        f\"'{best_model_path}'\"\n",
    "    )\n",
    "\n",
    "    return best_model_path\n",
    "\n",
    "\n",
    "def _train_loop(\n",
    "    cavity_model_net: CavityModel,\n",
    "    dataloader_train: DataLoader,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    "    matfact_k: int,\n",
    "    output_shape: int,\n",
    "    ) -> (torch.Tensor, float):\n",
    "    \"\"\"\n",
    "    Helper function to perform a train loop\n",
    "    \"\"\"\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    loss_batch_list = []\n",
    "\n",
    "    idx_res_split = output_shape // 2\n",
    "\n",
    "    cavity_model_net.train()\n",
    "    for batch_x, batch_y in tqdm.tqdm(dataloader_train,\n",
    "                                      total=len(dataloader_train),\n",
    "                                      unit=\"batch\",\n",
    "                                    ):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_y_pred = cavity_model_net(batch_x)\n",
    "\n",
    "        # Split predictions in (20, k) x (20, k) for matrix factorization\n",
    "        batch_y_pred_res1 = batch_y_pred[:, :idx_res_split].reshape(\n",
    "            -1, 20, matfact_k)\n",
    "        batch_y_pred_res2 = batch_y_pred[:, idx_res_split:].reshape(\n",
    "            -1, matfact_k, 20)\n",
    "\n",
    "        batch_y_pred = (batch_y_pred_res1 @ batch_y_pred_res2).reshape(-1, 400)\n",
    "\n",
    "        loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=1))\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_batch_list.append(loss_batch.detach().cpu().item())\n",
    "\n",
    "        # Save joint, conditional, marginal, restype accuracies.\n",
    "        labels_true.append(\n",
    "            np.vstack(np.unravel_index(\n",
    "                torch.argmax(batch_y, dim=1).detach().cpu().numpy(),\n",
    "                (20, 20)\n",
    "                )).T\n",
    "        )\n",
    "\n",
    "        labels_pred.append(\n",
    "            np.vstack(np.unravel_index(\n",
    "                torch.argmax(batch_y_pred, dim=1).detach().cpu().numpy(),\n",
    "                (20, 20)\n",
    "                )).T\n",
    "        )\n",
    "\n",
    "    loss_train = np.mean(loss_batch_list)\n",
    "\n",
    "    return (loss_train, _get_accuracies(\n",
    "                                    labels_true,\n",
    "                                    labels_pred)\n",
    "    )\n",
    "\n",
    "\n",
    "def _eval_loop(\n",
    "    cavity_model_net: CavityModel,\n",
    "    dataloader_val: DataLoader,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    "    matfact_k: int,\n",
    "    output_shape: int,\n",
    "    **kwargs\n",
    "    ) -> Tuple:\n",
    "    \"\"\"\n",
    "    Helper function to perform an eval loop\n",
    "    \"\"\"\n",
    "    # Eval loop. Due to memory, we don't pass the whole eval set to the model\n",
    "\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    loss_batch_list = []\n",
    "\n",
    "    idx_res_split = output_shape // 2\n",
    "\n",
    "    cavity_model_net.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch_x, batch_y in tqdm.tqdm(dataloader_val,\n",
    "                                        total=len(dataloader_val),\n",
    "                                        unit=\"batch\",\n",
    "                                        leave=False\n",
    "                                        ):\n",
    "            batch_y_pred = cavity_model_net(batch_x)\n",
    "\n",
    "            # Split predictions in (20, k) x (20, k) for matrix factorization\n",
    "            batch_y_pred_res1 = batch_y_pred[:, :idx_res_split].reshape(\n",
    "                -1, 20, matfact_k)\n",
    "            batch_y_pred_res2 = batch_y_pred[:, idx_res_split:].reshape(\n",
    "                -1, matfact_k, 20)\n",
    "\n",
    "            batch_y_pred = (batch_y_pred_res1 @ batch_y_pred_res2).reshape(\n",
    "                -1, 400)\n",
    "\n",
    "            loss_batch = loss_function(\n",
    "                batch_y_pred, torch.argmax(batch_y, dim=1))\n",
    "            loss_batch_list.append(loss_batch.detach().cpu().item())\n",
    "\n",
    "            # Save joint, conditional, marginal, restype accuracies.\n",
    "            labels_true.append(\n",
    "                np.vstack(np.unravel_index(\n",
    "                    torch.argmax(batch_y, dim=1).detach().cpu().numpy(),\n",
    "                    (20, 20)\n",
    "                    )).T\n",
    "            )\n",
    "\n",
    "            labels_pred.append(\n",
    "                np.vstack(np.unravel_index(\n",
    "                    torch.argmax(batch_y_pred, dim=1).detach().cpu().numpy(),\n",
    "                    (20, 20)\n",
    "                    )).T\n",
    "            )\n",
    "\n",
    "    loss_val = np.mean(loss_batch_list)\n",
    "\n",
    "    # return (loss_val, labels_true, labels_pred)\n",
    "    return (loss_val, _get_accuracies( # Unpack tuple of accuracies.\n",
    "                                labels_true,\n",
    "                                labels_pred,\n",
    "                                **kwargs)\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_accuracies(labels_true: List[int],\n",
    "                    labels_pred: List[int],\n",
    "                    get_restypes_acc=False,\n",
    "                    keep_pair_order=True,\n",
    "    ):\n",
    "    \"\"\" \n",
    "    compute join, marginal, conditional and restype accuracies\n",
    "    from lists of true and predicted labels.\n",
    "    \"\"\"\n",
    "    rec = dict()\n",
    "\n",
    "    # Create arrays from lists.\n",
    "    # labels_true = np.reshape(labels_true, (-1, 2))\n",
    "    labels_true = np.vstack(labels_true)\n",
    "    # labels_pred = np.reshape(labels_pred, (-1, 2))\n",
    "    labels_pred = np.vstack(labels_pred)\n",
    "\n",
    "    # joint accuracy\n",
    "    mask_join = np.logical_and(labels_true[:, 0] == labels_pred[:, 0],\n",
    "                               labels_true[:, 1] == labels_pred[:, 1])\n",
    "\n",
    "    rec[\"acc_join\"] = np.mean(mask_join)\n",
    "\n",
    "    # marginal accuracies\n",
    "\n",
    "    mask_res1_true = (labels_true[:, 0] == labels_pred[:, 0])\n",
    "    mask_res2_true = (labels_true[:, 1] == labels_pred[:, 1])\n",
    "\n",
    "    rec[\"acc_res1\"] = np.mean(mask_res1_true)\n",
    "    rec[\"acc_res2\"] = np.mean(mask_res2_true)\n",
    "\n",
    "    # conditional accuracies\n",
    "\n",
    "    mask_res1_true = mask_res1_true.nonzero()\n",
    "    mask_res2_true = mask_res2_true.nonzero()\n",
    "    \n",
    "    rec[\"acc_res2_given_res1\"] = np.mean(\n",
    "        labels_true[mask_res1_true, 1] == labels_pred[mask_res1_true, 1]\n",
    "        )\n",
    "    rec[\"acc_res1_given_res2\"] = np.mean(\n",
    "        labels_true[mask_res2_true, 0] == labels_pred[mask_res2_true, 0]\n",
    "        )\n",
    "\n",
    "    if get_restypes_acc:\n",
    "        # save restypes accuracies\n",
    "        pairres_count_true = np.zeros((20, 20), dtype=np.int)\n",
    "        pairres_count = np.zeros((20, 20), dtype=np.int)\n",
    "        pairres_count_true = np.zeros((20, 20), dtype=np.int)\n",
    "        pairres_count = np.zeros((20, 20), dtype=np.int)\n",
    "\n",
    "        # Get accuracies per residue type.\n",
    "        mask_join = mask_join.nonzero()\n",
    "        \n",
    "        if keep_pair_order:\n",
    "        # if order matters (can retrieve easily marginals):\n",
    "            mask_pairres_count_true, count_true = np.unique(\n",
    "                labels_pred[mask_join],\n",
    "                return_counts=True, axis=0\n",
    "                )\n",
    "            mask_pairres_count, count = np.unique(\n",
    "                labels_pred, return_counts=True, axis=0\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # if order does not matter:\n",
    "            mask_pairres_count_true, count_true = np.unique(\n",
    "                np.sort(labels_pred[mask_join], axis=1),\n",
    "                return_counts=True, axis=0\n",
    "                )\n",
    "            mask_pairres_count, count = np.unique(\n",
    "                np.sort(labels_pred, axis=1),\n",
    "                return_counts=True, axis=0\n",
    "                )\n",
    "\n",
    "        pairres_count_true[mask_pairres_count_true[:, 0],\n",
    "                        mask_pairres_count_true[:, 1]] += count_true\n",
    "        pairres_count[mask_pairres_count[:, 0],\n",
    "                    mask_pairres_count[:, 1]] += count\n",
    "\n",
    "        pairres_count[pairres_count == 0] = 1 # avoid 0 division.\n",
    "\n",
    "        rec[\"pairres_count_true\"] = pairres_count_true\n",
    "        rec[\"pairres_count\"] = pairres_count\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return rec\n",
    "\n",
    "\n",
    "def _train_val_split(\n",
    "    parsed_pdb_filenames: List[str],\n",
    "    TRAIN_VAL_SPLIT: float,\n",
    "    DEVICE: str,\n",
    "    BATCH_SIZE: int,\n",
    "    **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Helper function to perform training and validation split of ResidueEnvironments. Note that\n",
    "    we do the split on PDB level not on ResidueEnvironment level due to possible leakage.\n",
    "    \"\"\"\n",
    "    n_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\n",
    "    filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "    filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "    to_tensor_transformer = ToTensor(DEVICE, **kwargs) # allow for unravel indexing\n",
    "\n",
    "    dataset_train = ResidueEnvironmentsDataset(\n",
    "        filenames_train, transformer=to_tensor_transformer # thanks to call function\n",
    "    )\n",
    "\n",
    "\n",
    "    dataloader_train = DataLoader( # read the data (and shuffle it) within batch size and put into memory.\n",
    "        dataset_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=to_tensor_transformer.collate_cat, # avoid having to load data to CUDA in the NN model itself!\n",
    "        # collate_fn=to_tensor_transformer.collate_wrapper,\n",
    "        drop_last=True, # drop_last=True parameter ignores the last batch (when the number of examples in your dataset is not divisible by your batch_size\n",
    "        # pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Training data set includes {len(filenames_train)} pdbs with \"\n",
    "        f\"{len(dataset_train)} environments.\"\n",
    "    )\n",
    "\n",
    "    # dataset_train = 0\n",
    "\n",
    "    dataset_val = ResidueEnvironmentsDataset(\n",
    "        filenames_val, transformer=to_tensor_transformer\n",
    "    )\n",
    "\n",
    "    # TODO: Fix it so drop_last doesn't have to be True when calculating validation accuracy.\n",
    "    dataloader_val = DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=to_tensor_transformer.collate_cat, # if using /batch, callable that specifies how the batch is created.\n",
    "        # collate_fn=to_tensor_transformer.collate_wrapper,\n",
    "        drop_last=True, # ignores the last batch (when the number of examples in your dataset is not divisible by your batch_size\n",
    "        # pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Validation data set includes {len(filenames_val)} pdbs with \"\n",
    "        f\"{len(dataset_val)} environments.\"\n",
    "    )\n",
    "\n",
    "    # dataset_val = 0\n",
    "\n",
    "    return dataloader_train, dataset_train, dataloader_val, dataset_val\n",
    "\n",
    "\n",
    "def get_test_dataloader(\n",
    "    test_filenames: List[str],\n",
    "    BATCH_SIZE: int,\n",
    "    DEVICE: str,\n",
    "    reshape_index=True,\n",
    "    unravel_index=True,    \n",
    "    ):\n",
    "    \"\"\"Return a dataloder for testing.\"\"\"\n",
    "    to_tensor_transformer = ToTensor(DEVICE,\n",
    "                                     unravel_index=unravel_index,\n",
    "                                     reshape_index=reshape_index)\n",
    "\n",
    "    dataset_test = ResidueEnvironmentsDataset(\n",
    "        test_filenames,\n",
    "        transformer=to_tensor_transformer\n",
    "        )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=to_tensor_transformer.collate_cat,\n",
    "        drop_last=False,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Testing data set includes {len(test_filenames)} pdbs with \"\n",
    "        f\"{len(dataset_test)} environments.\"\n",
    "    )    \n",
    "\n",
    "    return dataset_test, dataloader_test\n",
    "\n",
    "\n",
    "def _test(\n",
    "    cavity_model_net: CavityModel,\n",
    "    dataloader_test: DataLoader,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    "    matfact_k: int,\n",
    "    output_shape: int,\n",
    "    get_restypes_acc=True,\n",
    "    keep_pair_order=True,\n",
    "):\n",
    "    return _eval_loop(cavity_model_net,\n",
    "                      dataloader_test,\n",
    "                      loss_function,\n",
    "                      matfact_k,\n",
    "                      output_shape,\n",
    "                      get_restypes_acc=get_restypes_acc,\n",
    "                      keep_pair_order=keep_pair_order,\n",
    "                      )\n",
    "\n",
    "\n",
    "def _predict(\n",
    "    cavity_model_net: CavityModel,\n",
    "    dataloader_infer: DataLoader,\n",
    "    matfact_k: int,\n",
    "    output_shape: int,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Get predicted proba distribution per pair_res environment.\n",
    "    Made for making prediction one protein at a time, returning an array\n",
    "    (n_pairs, 400) long.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_true = []\n",
    "    idx_res_split = output_shape // 2\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    cavity_model_net.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch_x, batch_y in dataloader_infer:\n",
    "\n",
    "            batch_y_pred = cavity_model_net(batch_x)\n",
    "\n",
    "            # Split predictions in (20, k) x (20, k) for matrix factorization\n",
    "            batch_y_pred_res1 = batch_y_pred[:, :idx_res_split].reshape(\n",
    "                -1, 20, matfact_k)\n",
    "            batch_y_pred_res2 = batch_y_pred[:, idx_res_split:].reshape(\n",
    "                -1, matfact_k, 20)\n",
    "\n",
    "            batch_y_pred = (batch_y_pred_res1 @ batch_y_pred_res2).reshape(\n",
    "                -1, 400)\n",
    "            batch_y_pred = softmax(batch_y_pred).detach().cpu().numpy()\n",
    "\n",
    "            # Save true labels\n",
    "            labels_true.append(\n",
    "                np.vstack(np.unravel_index(\n",
    "                    torch.argmax(batch_y, dim=1).detach().cpu().numpy(),\n",
    "                    (20, 20)\n",
    "                    )).T\n",
    "            )\n",
    "\n",
    "    return batch_y_pred, labels_true\n",
    "\n",
    "\n",
    "def get_best_epoch_perf(model_name: str= \"\",\n",
    "                        models_dirpath=\"models/double_cav_models/\"):\n",
    "    \"\"\"\n",
    "    Fetch training history of model_name,\n",
    "    return results of the best epoch as string.\n",
    "    \"\"\"\n",
    "    with open(f\"{models_dirpath}/metrics_{model_name}.pickle\", \"rb\") as f:\n",
    "        history = pickle.load(f)\n",
    "    best_epoch = history.pop(\"best_epoch\")\n",
    "    best_epoch_perf = {}\n",
    "    for key in history:\n",
    "        best_epoch_perf[key] = {}\n",
    "        for metric in history[key]:\n",
    "            best_epoch_perf[key][metric] = history[key][metric][best_epoch]\n",
    "\n",
    "    best_epoch_perf = f\"Best epoch: {best_epoch}\\n\"\\\n",
    "    f\"{'train'.upper():5s} - \"\\\n",
    "    f\"Loss: {best_epoch_perf['train']['loss']:5.3f}, \"\\\n",
    "    f\"Join acc: {best_epoch_perf['train']['acc_join']:5.3f},  \"\\\n",
    "    f\"Res1: {best_epoch_perf['train']['acc_res1']:4.2f}, \"\\\n",
    "    f\"Res2: {best_epoch_perf['train']['acc_res2']:4.2f},  \"\\\n",
    "    f\"Cond 2|1: {best_epoch_perf['train']['acc_res2_given_res1']:4.2f}, \"\\\n",
    "    f\"Cond 1|2: {best_epoch_perf['train']['acc_res1_given_res2']:4.2f}.\"\\\n",
    "    f\"\\n\"\\\n",
    "    f\"{'val'.upper():5s} - Loss: {best_epoch_perf['val']['loss']:5.3f}, \"\\\n",
    "    f\"Join acc: {best_epoch_perf['val']['acc_join']:5.3f},  \"\\\n",
    "    f\"Res1: {best_epoch_perf['val']['acc_res1']:4.2f}, \"\\\n",
    "    f\"Res2: {best_epoch_perf['val']['acc_res2']:4.2f},  \"\\\n",
    "    f\"Cond 2|1: {best_epoch_perf['val']['acc_res2_given_res1']:4.2f}, \"\\\n",
    "    f\"Cond 1|2: {best_epoch_perf['val']['acc_res1_given_res2']:4.2f}.\"\n",
    "\n",
    "    return best_epoch_perf\n",
    "\n",
    "\n",
    "# Tools for saving model summary (objective: combine the 2!)\n",
    "def get_df_summary(text: str):\n",
    "    from torchsummary import summary\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "    def parse_line(line: list):\n",
    "        parsed_line = []\n",
    "        for el in line:\n",
    "            if not el == \"\":\n",
    "                parsed_line.append(el)\n",
    "        return parsed_line\n",
    "\n",
    "    keys = [\"Layer (type)\", \"Output shape\", \"Param #\"]\n",
    "    df_summary = {k: [] for k in keys}\n",
    "\n",
    "    for line in text:\n",
    "        line = parse_line(line.split(\"  \"))\n",
    "        df_summary[\"Layer (type)\"].append(line[0])\n",
    "        df_summary[\"Output shape\"].append(line[1])\n",
    "        df_summary[\"Param #\"].append(line[2])\n",
    "    return pd.DataFrame(df_summary)\n",
    "\n",
    "\n",
    "def get_and_save_model_summary(model: CavityModel,\n",
    "                               input_size: tuple,\n",
    "                               model_name: \"cavity_model\"):\n",
    "    import io\n",
    "    from torchsummary import summary\n",
    "    from contextlib import redirect_stdout\n",
    "\n",
    "    # Context manager for temporarily redirecting sys.stdout to another file or file-like object.\n",
    "    with open(f'models/{model_name}_summary.txt', 'w') as f:\n",
    "        f = io.StringIO()\n",
    "        with redirect_stdout(f):\n",
    "            summary(model=model, input_size=input_size)\n",
    "        out = f.getvalue()\n",
    "        return out\n",
    "\n",
    "\n",
    "# Tools for sending run's completion notification\n",
    "def test_login_smtp_server(\n",
    "    sender_email = \".@gmail.com\",\n",
    "    receiver_email = \".@gmail.com\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    In case of error 534: # https://accounts.google.com/DisplayUnlockCaptcha\n",
    "    Debug ref: https://stackoverflow.com/questions/16512592/login-credentials-not-working-with-gmail-smtp\n",
    "    \"\"\"\n",
    "    port = 587  # For starttls (tls encryption protocol)\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "\n",
    "    password = getpass(\"Type password: \")\n",
    "\n",
    "    # Create a secure SSL context\n",
    "    context = ssl.create_default_context()\n",
    "\n",
    "    # Try to log in to server and send email\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, port)\n",
    "        server.ehlo() # Can be omitted\n",
    "        server.starttls(context=context) # Secure the connection\n",
    "        server.ehlo() # Can be omitted\n",
    "        server.login(sender_email, password)\n",
    "        # TODO: Send email here\n",
    "    except Exception as e:\n",
    "        # Print any error messages to stdout\n",
    "        print(e)\n",
    "    finally:\n",
    "        server.quit()\n",
    "    return password\n",
    "\n",
    "\n",
    "def send_run_results(\n",
    "    h: dict,\n",
    "    password: str,\n",
    "    models_dirpath=\"models/double_cav_models/\",\n",
    "    sender_email=\".@gmail.com\",\n",
    "    receiver_email=\".@gmail.com\",\n",
    "    ):\n",
    "\n",
    "    port = 587  # For starttls (tls encryption protocol)\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "\n",
    "    subject_email = f\"run of Model: {h['model_name']} completed.\"\n",
    "\n",
    "    perf_best_epoch = get_best_epoch_perf(h[\"model_name\"],\n",
    "                                          models_dirpath=models_dirpath)\n",
    "\n",
    "    hyperparam_records = f\"\"\"Hyperparameters:\n",
    "    ----------------\n",
    "    \"\"\"\n",
    "    for key in h:\n",
    "        if key != \"model_name\":\n",
    "            hyperparam_records += f\"{key}: {h[key]}\\n\"\n",
    "    \n",
    "    message = \"\"\"\\\n",
    "    From: {}\n",
    "    To: {}\n",
    "    Subject: {}\n",
    "\n",
    "    {}\n",
    "\n",
    "    {}\"\"\".format(\n",
    "        sender_email,\n",
    "        receiver_email,\n",
    "        subject_email,\n",
    "        perf_best_epoch,\n",
    "        hyperparam_records\n",
    "        )\n",
    "    message = \"\\n\".join([line.lstrip() for line in message.split(\"\\n\")])\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP(smtp_server, port) as server:\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.starttls(context=context)\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)\n",
    "\n",
    "        print(f\"Email sent to {receiver_email}.\")\n",
    "\n",
    "\n",
    "def send_run_results(\n",
    "    h: dict,\n",
    "    password: str,\n",
    "    models_dirpath=\"models/double_cav_models/\",\n",
    "    sender_email=\".@gmail.com\",\n",
    "    receiver_email=\".@gmail.com\",\n",
    "    ):\n",
    "\n",
    "    port = 587  # For starttls (tls encryption protocol)\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "\n",
    "    subject_email = f\"run of Model: {h['model_name']} completed.\"\n",
    "\n",
    "    perf_best_epoch = get_best_epoch_perf(h[\"model_name\"],\n",
    "                                          models_dirpath=models_dirpath)\n",
    "\n",
    "    hyperparam_records = f\"\"\"Hyperparameters:\n",
    "    ----------------\n",
    "    \"\"\"\n",
    "    for key in h:\n",
    "        if key != \"model_name\":\n",
    "            hyperparam_records += f\"{key}: {h[key]}\\n\"\n",
    "    \n",
    "    message = \"\"\"\\\n",
    "    From: {}\n",
    "    To: {}\n",
    "    Subject: {}\n",
    "\n",
    "    {}\n",
    "\n",
    "    {}\"\"\".format(\n",
    "        sender_email,\n",
    "        receiver_email,\n",
    "        subject_email,\n",
    "        perf_best_epoch,\n",
    "        hyperparam_records\n",
    "        )\n",
    "    message = \"\\n\".join([line.lstrip() for line in message.split(\"\\n\")])\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP(smtp_server, port) as server:\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.starttls(context=context)\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)\n",
    "\n",
    "        print(f\"Email sent to {receiver_email}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbD3OZGKe9Km"
   },
   "source": [
    "# Parse .pdb files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDnpPvOifFz8"
   },
   "outputs": [],
   "source": [
    "from pdb_parser_scripts.clean_pdb import clean_pdb\n",
    "import traceback\n",
    "\n",
    "# !chmod +x reduce/reduce\n",
    "# !chmod +x pdb_parser_scripts/clean_pdb.py\n",
    "# !chmod +x pdb_parser_scripts/extract_pair_environments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(\"data/pdbs/raw_tm/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(\"data/pdbs/cleaned\")\n",
    "    os.makedirs(\"data/pdbs/parsed\")\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdb_files = [\"\"]\n",
    "\n",
    "fails = []\n",
    "for i, pdb_filename in enumerate(pdb_files):\n",
    "    try:\n",
    "        clean_pdb(pdb_filename, \"data/pdbs/cleaned\", \"/reduce/reduce\")\n",
    "        pdb_filename = os.path.basename(pdb_filename)\n",
    "        print(f\"{pdb_filename} cleaned successfully ({i+1}/{len(pdb_files)})\")\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"{pdb_filename} failed. Nb: {i+1}.\")\n",
    "        fails.append(pdb_filename)\n",
    "\n",
    "        error_msg = traceback.format_exc()\n",
    "        print(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_files = [\"\", \"\"]\n",
    "seq = {}\n",
    "res_nb = {}\n",
    "for pdb_file, tag in zip(pdb_files, [\"str\", \"seq\"]):\n",
    "    print(os.path.basename(pdb_file))\n",
    "    pdb_simtk = simtk.openmm.app.PDBFile(pdb_file)\n",
    "    seq[tag] = []\n",
    "    res_nb[tag] = []\n",
    "    for chain in pdb_simtk.getTopology().chains():\n",
    "        for res in chain.residues():\n",
    "            try:\n",
    "                seq[tag].append(Bio.PDB.Polypeptide.three_to_one(res.name))\n",
    "                res_nb[tag].append(res.id)\n",
    "            except:\n",
    "                print(\"error for\", res.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = np.ones_like(seq[\"seq\"], dtype=np.bool)\n",
    "mask[0] = 0\n",
    "mask[90:99] = 0\n",
    "\n",
    "compatibility_table = pd.DataFrame({\"seq_str\": np.array(seq[\"str\"]), \"seq_seq\": np.array(seq[\"seq\"])[mask][:-1],\n",
    "              \"res_nb_str\": np.array(res_nb[\"str\"], dtype=int)-20,\n",
    "              \"res_nb_seq\": np.array(res_nb[\"seq\"], dtype=int)[mask][:-1]-2})\n",
    "\n",
    "# create a number correction mapper\n",
    "nb_mapper = {}\n",
    "for wrong_nb, corr_nb in zip(list(compatibility_table.index), compatibility_table.res_nb_str):\n",
    "    nb_mapper[wrong_nb] = int(corr_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kbj__1wj3RSF"
   },
   "outputs": [],
   "source": [
    "from pdb_parser_scripts import (\n",
    "    extract_pair_environments,\n",
    "    grid\n",
    ")\n",
    "\n",
    "pdb_files = sorted(glob.glob(\"data/pdbs/cleaned/*_clean.pdb\"))\n",
    "\n",
    "OUT_DIR = \"data/pdbs/parsed_325\"\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "fails=[]\n",
    "for i, filename in enumerate(pdb_files):\n",
    "    pdb_filename = os.path.basename(filename)\n",
    "    pdb_id = pdb_filename.split(\".\")[0]\n",
    "    try:\n",
    "        extract_pair_environments.extract_environments(filename,\n",
    "                                                       pdb_id,\n",
    "                                                       out_dir=OUT_DIR,\n",
    "                                                       max_width_x=4.5,\n",
    "                                                       max_width_y=4.5,\n",
    "                                                       max_height=9.0,\n",
    "                                                    #    ca_ca_cutoff=7.0,\n",
    "                                                       ca_ca_dist_based=False,\n",
    "                                                       max_radius=3.25,\n",
    "                                                       min_radius=0,\n",
    "                                                       )\n",
    "        print(f\"{pdb_filename} successful ({i+1}/{len(pdb_files)})\")\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"{pdb_filename} failed. Nb: {i+1}.\")\n",
    "        fails.append(filename)\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLTLVNCt4dUz"
   },
   "outputs": [],
   "source": [
    "from pdb_parser_scripts import (\n",
    "    extract_pair_environments,\n",
    "    grid\n",
    ")\n",
    "\n",
    "pdb_files = sorted(glob.glob(\"data/pdbs/cleaned/*_clean.pdb\"))\n",
    "\n",
    "OUT_DIR = \"data/pdbs/parsed_450\"\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "fails=[]\n",
    "for i, filename in enumerate(pdb_files):\n",
    "    pdb_filename = os.path.basename(filename)\n",
    "    pdb_id = pdb_filename.split(\".\")[0]\n",
    "    try:\n",
    "        extract_pair_environments.extract_environments(filename,\n",
    "                                                       pdb_id,\n",
    "                                                       out_dir=OUT_DIR,\n",
    "                                                       max_width_x=4.5,\n",
    "                                                       max_width_y=4.5,\n",
    "                                                       max_height=9.0,\n",
    "                                                    #    ca_ca_cutoff=7.0,\n",
    "                                                       ca_ca_dist_based=False,\n",
    "                                                       max_radius=4.5,\n",
    "                                                       min_radius=0,\n",
    "                                                       )\n",
    "        print(f\"{pdb_filename} successful ({i+1}/{len(pdb_files)})\")\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"{pdb_filename} failed. Nb: {i+1}.\")\n",
    "        fails.append(filename)\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssbonds = pd.read_table(\"data/tm_results.csv\", delimiter=\";\")\n",
    "ssbonds.rename(columns={\"Tm (Celcius)\": \"tm\", \"construct_planned_mutations\": \"ss_mut\"}, inplace=True)\n",
    "ssbonds.drop(index=0, inplace=True)\n",
    "ssbonds = ssbonds.reset_index(drop=True)\n",
    "\n",
    "# convert the strings \"[res1_id, res2_id]\" to separate columns, and proper tuple of res_id.\n",
    "ssbonds_res = ssbonds.ss_mut.str.extractall(\"'(?P<res1_id_temp>.*)', '(?P<res2_id_temp>.*)'\").reset_index()[[\"res1_id_temp\", \"res2_id_temp\"]]\n",
    "ssbonds[[\"res1_id_temp\", \"res2_id_temp\"]] = ssbonds_res\n",
    "\n",
    "for i in range(2):\n",
    "    ssbonds[f\"res{i+1}_id\"] = ssbonds[f\"res{i+1}_id_temp\"].apply(\n",
    "        lambda x: [\n",
    "            Bio.PDB.Polypeptide.one_to_three(x[0]),\n",
    "            int(x[1:-1]),\n",
    "            \"A\"\n",
    "        ])\n",
    "\n",
    "    ssbonds[f\"res{i+1}\"] = ssbonds[f\"res{i+1}_id\"].apply(\n",
    "        lambda x: \n",
    "            Bio.PDB.Polypeptide.three_to_index(x[0])\n",
    "    )\n",
    "\n",
    "    ssbonds[f\"res{i+1}_target\"] = ssbonds[f\"res{i+1}_id_temp\"].apply(\n",
    "        lambda x: x[-1])\n",
    "\n",
    "    ssbonds.drop(columns=f\"res{i+1}_id_temp\", inplace=True)\n",
    "\n",
    "# ADD ASSERTION THAT FOR ALL PAIRS, ID FOR RES1 IS < ID RES2 (otherwise invert them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssbonds.to_pickle(f\"data/tm_ssbonds_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/nb_mapper.pickle\", \"wb\") as f:\n",
    "    pickle.dump(nb_mapper, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cavity model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if \"CUDA out of memory\" issues: switch to another GPU BEFORE importing torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "# import torch\n",
    "# available_gpus = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "# print(available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssbonds = pd.read_pickle(\"data/tm_ssbonds_table\")\n",
    "\n",
    "with open(\"data/nb_mapper.pickle\", \"rb\") as f:\n",
    "    nb_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/nb_mapper.pickle\", \"rb\") as f:\n",
    "    nb_mapper = pickle.load(f)\n",
    "inv_mapper = {v: k for k, v in nb_mapper.items()}\n",
    "assert len(inv_mapper) == len(nb_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssbonds[\"pdb_id\"] = \"gcl1_apo_final_york_experimental_structure\"\n",
    "ssbonds[\"ss\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_name = \"m1_bigger_backbone_b1000_5atoms_matfact_1_lr_2\" # parsed_450\n",
    "# dset_name = \"parsed_450\"\n",
    "# model_name = \"m1_bigger_backbone_b1000_5atoms_matfact_1_lr_3_q_1\" # parsed_325 (no weights)\n",
    "dset_name = \"parsed_325\"\n",
    "model_name = \"m1_bigger_backbone_b1000_5atoms_matfact_1_lr_3_q_1_weighted\" # parsed_325 (weighted)\n",
    "\n",
    "# model_name = \"idp_325_1\"\n",
    "\n",
    "MODELS_DIRPATH = \"models/double_cav_models\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "with open(f\"{MODELS_DIRPATH}/{model_name}_metadata.pickle\", \"rb\") as f:\n",
    "    h = pickle.load(f)\n",
    "\n",
    "with open(f\"{MODELS_DIRPATH}/metrics_{model_name}.pickle\", \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "best_epoch = history[\"best_epoch\"]\n",
    "print(\"Best epoch:\", best_epoch)\n",
    "model_state = torch.load(f\"{MODELS_DIRPATH}/{model_name}_epoch_{best_epoch}.pt\")[\"model_state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_all_pairs = {}\n",
    "ss_pairs_mask = {}\n",
    "\n",
    "for pdb_id, res1, res2 in zip(ssbonds.pdb_id, ssbonds.res1_id, ssbonds.res2_id):\n",
    "\n",
    "    filename = f\"data/pdbs/{dset_name}/{pdb_id}_clean_0_pair_res_features.npz\"\n",
    "    file_ = np.load(filename)\n",
    "\n",
    "    try:\n",
    "        mask = np.logical_and(file_[\"pair_res_indices\"][:, 0] == inv_mapper[res1[1]],\n",
    "                              file_[\"pair_res_indices\"][:, 1] == inv_mapper[res2[1]])\n",
    "    except KeyError:\n",
    "        print(f\"{res1[1]} or {res2[1]} not included in .pdb structure.\")\n",
    "\n",
    "    if pdb_id not in pred_all_pairs:\n",
    "        pred_all_pairs[pdb_id] = {}\n",
    "        ss_pairs_mask[pdb_id] = []\n",
    "\n",
    "        test_dataset, test_dataloader = get_test_dataloader(\n",
    "                                            [filename],\n",
    "                                            BATCH_SIZE=file_[\"pair_res_indices\"].shape[0],\n",
    "                                            DEVICE=DEVICE,\n",
    "                                            reshape_index=False,\n",
    "                                                    )\n",
    "        # Define model\n",
    "        cavity_model_net = h[\"cav_model\"](DEVICE, \n",
    "                                        grid_dim_xy=h[\"grid_dim_xy\"],\n",
    "                                        grid_dim_z=h[\"grid_dim_z\"],\n",
    "                                        n_atom_types=h[\"n_atom_types\"]).to(DEVICE)\n",
    "\n",
    "        cavity_model_net.load_state_dict(model_state)\n",
    " \n",
    "        preds, labels_true = _predict(\n",
    "            cavity_model_net,\n",
    "            test_dataloader,\n",
    "            h[\"matfact_k\"],\n",
    "            h[\"output_shape\"])\n",
    "\n",
    "        labels_true = np.reshape(labels_true, (-1, 2))\n",
    "\n",
    "        pred_all_pairs[pdb_id][\"preds\"] = preds\n",
    "        pred_all_pairs[pdb_id][\"labels_true\"] = labels_true\n",
    "\n",
    "\n",
    "    try:\n",
    "        ss_pairs_mask[pdb_id].append(mask.nonzero()[0][0])\n",
    "    except (IndexError, KeyError):\n",
    "#         ss_pairs_mask[pdb_id].append(None)\n",
    "        print(\n",
    "            f\"ssbond pair ({res1[1]}-{res2[1]}) not found. \"\n",
    "#             f\"mapped pair indices: {inv_mapper[res1[1]]}-{inv_mapper[res2[1]]}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Create a dataframe per pdb_id and concatenate them.\n",
    "df = []\n",
    "for i, pdb_id in enumerate(pred_all_pairs):\n",
    "    df.append(pd.DataFrame({\n",
    "        \"pdb_id\": pdb_id,\n",
    "        \"res1\": [label[0] for label in pred_all_pairs[pdb_id][\"labels_true\"]],\n",
    "        \"res2\":  [label[1] for label in pred_all_pairs[pdb_id][\"labels_true\"]],\n",
    "        \"preds\": [pred for pred in pred_all_pairs[pdb_id][\"preds\"]],\n",
    "        \"ss\": 0,\n",
    "    })\n",
    "    )\n",
    "    try:\n",
    "        df[i].loc[ss_pairs_mask[pdb_id], \"ss\"] = 1\n",
    "    except (IndexError, KeyError):\n",
    "        pass\n",
    "\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_all_pairs = {}\n",
    "ss_pairs_mask = {}\n",
    "\n",
    "for pdb_id, res1, res2 in zip(ssbonds.pdb_id, ssbonds.res1_id, ssbonds.res2_id):\n",
    "\n",
    "    filename = f\"data/pdbs/{dset_name}/{pdb_id}_clean_1_pair_res_features.npz\"\n",
    "    file_ = np.load(filename)\n",
    "\n",
    "    try:\n",
    "        mask = np.logical_and(file_[\"pair_res_indices\"][:, 0] == inv_mapper[res2[1]],\n",
    "                              file_[\"pair_res_indices\"][:, 1] == inv_mapper[res1[1]])\n",
    "    except KeyError:\n",
    "        print(f\"{res1[1]} or {res2[1]} not included in .pdb structure.\")\n",
    "\n",
    "    if pdb_id not in pred_all_pairs:\n",
    "        pred_all_pairs[pdb_id] = {}\n",
    "        ss_pairs_mask[pdb_id] = []\n",
    "\n",
    "        test_dataset, test_dataloader = get_test_dataloader(\n",
    "                                            [filename],\n",
    "                                            BATCH_SIZE=file_[\"pair_res_indices\"].shape[0],\n",
    "                                            DEVICE=DEVICE,\n",
    "                                            reshape_index=False,\n",
    "                                                    )\n",
    "        # Define model\n",
    "        cavity_model_net = h[\"cav_model\"](DEVICE, \n",
    "                                        grid_dim_xy=h[\"grid_dim_xy\"],\n",
    "                                        grid_dim_z=h[\"grid_dim_z\"],\n",
    "                                        n_atom_types=h[\"n_atom_types\"]).to(DEVICE)\n",
    "\n",
    "        cavity_model_net.load_state_dict(model_state)\n",
    " \n",
    "        preds, labels_true = _predict(\n",
    "            cavity_model_net,\n",
    "            test_dataloader,\n",
    "            h[\"matfact_k\"],\n",
    "            h[\"output_shape\"])\n",
    "\n",
    "        labels_true = np.reshape(labels_true, (-1, 2))\n",
    "\n",
    "        pred_all_pairs[pdb_id][\"preds\"] = preds\n",
    "        pred_all_pairs[pdb_id][\"labels_true\"] = labels_true\n",
    "\n",
    "\n",
    "    try:\n",
    "        ss_pairs_mask[pdb_id].append(mask.nonzero()[0][0])\n",
    "    except (IndexError, KeyError):\n",
    "#         ss_pairs_mask[pdb_id].append(None)\n",
    "        print(\n",
    "            f\"ssbond pair ({res1[1]}-{res2[1]}) not found. \"\n",
    "#             f\"mapped pair indices: {inv_mapper[res1[1]]}-{inv_mapper[res2[1]]}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Create a dataframe per pdb_id and concatenate them.\n",
    "df_2 = []\n",
    "for i, pdb_id in enumerate(pred_all_pairs):\n",
    "    df_2.append(pd.DataFrame({\n",
    "        \"pdb_id\": pdb_id,\n",
    "        \"res1\": [label[0] for label in pred_all_pairs[pdb_id][\"labels_true\"]],\n",
    "        \"res2\":  [label[1] for label in pred_all_pairs[pdb_id][\"labels_true\"]],\n",
    "        \"preds_2\": [pred for pred in pred_all_pairs[pdb_id][\"preds\"]],\n",
    "        \"ss\": 0,\n",
    "    })\n",
    "    )\n",
    "    try:\n",
    "        df[i].loc[ss_pairs_mask[pdb_id], \"ss\"] = 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "df_2 = pd.concat(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average the predictions for the two copies per .pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.rename(columns={\"res2\": \"res1\", \"res1\": \"res2\"}, inplace=True)\n",
    "\n",
    "df[\"avg_preds\"] = (df.preds + df_2.preds_2) / 2\n",
    "\n",
    "del df_2\n",
    "\n",
    "df = df.drop(columns=[\"preds\"]).rename(columns={\"avg_preds\": \"preds\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge results with pairs of residues id.\n",
    "ssbonds = pd.merge(df[df.ss == 1], ssbonds, on=[\"ss\", \"pdb_id\", \"res1\", \"res2\"], left_index=True)\n",
    "ssbonds[\"pair_nb\"] = ssbonds.index\n",
    "\n",
    "# Save results.\n",
    "ssbonds.to_pickle(f\"data/ssbonds_preds_cav_{h['model_name']}\")\n",
    "df.to_pickle(f\"data/preds_cav_{h['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSBONDPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import warnings\n",
    "from scipy.spatial.distance import pdist\n",
    "import scipy.spatial.distance as ssd\n",
    "import time\n",
    "import math\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1) process_loadedpdb.py.......................................................................................\n",
    "\n",
    "def process_pdb(args, PositionOfThisProject):\n",
    "    name = args.split('/')[-1].split('.')[0]\n",
    "    map_list, map_id, mol_type_list = find_map_element(args)\n",
    "\n",
    "    if map_list == []:\n",
    "        print('no bonds')\n",
    "        return False\n",
    "\n",
    "    # Get the pairs of residues with CA-CA distce in [3, 7] angstrom.\n",
    "    possible_ssbond, possible_ssbond_id = make_ssbond_without_repeat(map_list, map_id, mol_type_list)\n",
    "    full_distance_map = convert_to_nxn_map(np.array(possible_ssbond))\n",
    "\n",
    "    # print('candidate bonds', len(full_distance_map))\n",
    "\n",
    "    predict_path = np.array(full_distance_map) # stack all 10x10 on axis 0 (n, 10, 10)\n",
    "    predict_ord_path = np.array(possible_ssbond_id) # (n, 2)\n",
    "\n",
    "    # Get probability and \n",
    "    result_dict = main([predict_path, predict_ord_path, name], PositionOfThisProject)\n",
    "\n",
    "    # noSG_restore_fnn.set_pointdir(PositionOfThisProject)\n",
    "    sorted_result_dict = sorted(result_dict.items(), key=operator.itemgetter(1), reverse=True) # key could be lambda x: x[1]\n",
    "    # final_dict = sorted_result_dict\n",
    "    final_dict = OrderedDict()\n",
    "    for item in sorted_result_dict:\n",
    "        final_dict[item[0]] = item[1]\n",
    "    return final_dict\n",
    "#read the result_dict and sav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) ssbond_distance_map.py.......................................................................................\n",
    "\n",
    "def convert_to_nxn_map(ssbonds_map):\n",
    "    errorcount = 0\n",
    "    ssbonds_distance_map = []\n",
    "    pos = 0\n",
    "    for smapi in range(len(ssbonds_map)):\n",
    "        try:\n",
    "            Y = pdist(ssbonds_map[smapi], 'euclidean') # 45-dimensional vector\n",
    "            ssbonds_distance_map.append(ssd.squareform(Y)) # convert distance vector to square matrix 10x10\n",
    "\n",
    "        except ValueError:\n",
    "            errorcount += 1\n",
    "\n",
    "            with open('ssbond_map_error.txt', 'a') as wf:\n",
    "                wf.write(str(ssbonds_map[smapi]))\n",
    "                wf.write('\\n')\n",
    "\n",
    "            for xyz in ssbonds_map[smapi]:\n",
    "                if abs(len(xyz[0])-len(xyz[1])) <= 1:\n",
    "                    continue\n",
    "                if len(xyz[0]) > len(xyz[1]):\n",
    "                    pos = 0\n",
    "                else:\n",
    "                    pos = 1\n",
    "                temp = xyz[pos].split('-')\n",
    "\n",
    "                if pos == 0:\n",
    "                    xyz[pos+2] = xyz[pos+1]\n",
    "                    if temp[0] == '':\n",
    "                        xyz[pos] = float('-'+temp[1])\n",
    "                        xyz[pos+1] = float('-'+temp[2])\n",
    "                    else:\n",
    "                        xyz[pos] = float(temp[0])\n",
    "                        xyz[pos+1] = float('-'+temp[1])\n",
    "                else:\n",
    "                    if temp[0] == '':\n",
    "                        xyz[pos] = float('-'+temp[1])\n",
    "                        xyz[pos+1] = float('-'+temp[2])\n",
    "                    else:\n",
    "                        xyz[pos] = float(temp[0])\n",
    "                        xyz[pos+1] = float('-'+temp[1])\n",
    "            with open('ssbond_map_correct_error.txt', 'a') as wcf:\n",
    "                wcf.write(str(ssbonds_map[smapi]))\n",
    "                wcf.write('\\n')\n",
    "                wcf.write('***************************************\\n')\n",
    "            Y = pdist(ssbonds_map[smapi], 'euclidean')\n",
    "            ssbonds_distance_map.append(ssd.squareform(Y))\n",
    "    return ssbonds_distance_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) extract_unknown_map.py.......................................................................................\n",
    "\n",
    "remove_pairs = open('small_ca_remove.txt', 'w')\n",
    "def compare_CA_distance(A_CA, B_CA, nameA, nameB):\n",
    "    sumCA = 0\n",
    "    for xyz in range(3):\n",
    "        sumCA += pow((float(A_CA[xyz])-float(B_CA[xyz])), 2)\n",
    "    distance = math.sqrt(sumCA)\n",
    "    if distance < 7: # comparison operators chaining\n",
    "        return True\n",
    "    else:\n",
    "        remove_pairs.write(nameA+', '+nameB+':'+str(distance) +'\\n')\n",
    "        return False\n",
    "\n",
    "\n",
    "def exmain_mol_list(mol_list, line):\n",
    "    if mol_list[0] != [] and mol_list[1] != [] and mol_list[2] != [] and mol_list[3] != [] and mol_list[4] != []:\n",
    "        return True, line[17:20].strip()+line[21]\n",
    "    else:\n",
    "        return False, line[17:20].strip()+line[21]\n",
    "\n",
    "def find_map_element(filename):\n",
    "    \"\"\"\n",
    "    Excludes Glycine in this preprocessing since it has no CB atom,.\n",
    "    and also excludes Proline.\"\"\"\n",
    "\n",
    "    mol_map_list = []\n",
    "    flag_mol = False\n",
    "    mol_id = None\n",
    "    mol_id_list = []\n",
    "    mol_name_temp = None\n",
    "    last_mol = None\n",
    "    mol_type_list = []\n",
    "    count = 0\n",
    "    break_count = 0\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            break_count += 1\n",
    "            line_tag = line[:6].strip()\n",
    "            if line_tag != 'ATOM':\n",
    "                continue\n",
    "#             if line[17:20].strip() == 'PRO':\n",
    "#                 continue\n",
    "            if line_tag == 'ENDMDL' :\n",
    "                break\n",
    "\n",
    "            residue = line[17:20].strip()+line[21]+line[22:26].strip()\n",
    "\n",
    "            # Initialize a residue.            \n",
    "            if line_tag == 'ATOM' and mol_name_temp == None:\n",
    "                mol_name_temp = residue\n",
    "                mol_list = [ [] for i in range(5)]\n",
    "            elif line_tag == 'ATOM' and mol_name_temp != residue: # new residue\n",
    "                mol_name_temp = residue\n",
    "                mol_list = [ [] for i in range(5)]\n",
    "                count += 1\n",
    "\n",
    "            # Save 3d coordinates of the 5 atoms of interest of this residue\n",
    "            if line_tag == 'ATOM':\n",
    "                if line[12:16].strip() == 'N' and mol_list[0] ==[]:\n",
    "                    mol_list[0]=[line[30:38].strip(), line[38:46].strip(), line[46:54].strip()]\n",
    "                    flag_mol, mol_id = exmain_mol_list(mol_list, line)\n",
    "                elif line[12:16].strip() =='CA' and mol_list[1] == []:\n",
    "                    mol_list[1]=[line[30:38].strip(), line[38:46].strip(), line[46:54].strip()]\n",
    "                    flag_mol, mol_id = exmain_mol_list(mol_list, line)\n",
    "                elif line[12:16].strip()  =='C' and mol_list[2] == []:\n",
    "                    mol_list[2]=[line[30:38].strip(), line[38:46].strip(), line[46:54].strip()]\n",
    "                    flag_mol, mol_id = exmain_mol_list(mol_list, line)\n",
    "                elif line[12:16].strip()  =='O' and mol_list[3] == []:\n",
    "                    mol_list[3]=[line[30:38].strip(), line[38:46].strip(), line[46:54].strip()]\n",
    "                    flag_mol, mol_id = exmain_mol_list(mol_list, line)\n",
    "                elif line[12:16].strip()  =='CB' and mol_list[4] == []:\n",
    "                    mol_list[4]=[line[30:38].strip(), line[38:46].strip(), line[46:54].strip()]\n",
    "                    flag_mol, mol_id = exmain_mol_list(mol_list, line)\n",
    "\n",
    "                # Append to list of residues\n",
    "                if flag_mol == True:\n",
    "                    mol_map_list.append(mol_list)\n",
    "                    mol_id_list.append(mol_id)\n",
    "                    mol_type_list.append(residue)\n",
    "                    flag_mol = False\n",
    "\n",
    "    return mol_map_list, mol_id_list, mol_type_list\n",
    "\n",
    "def make_ssbond_without_repeat(map_list, map_id, mol_type_list):\n",
    "    if len(map_list) != len(map_id):\n",
    "        # print('map list length is not equal to map id list!')\n",
    "        sys.exit()\n",
    "\n",
    "    possible_ssbond = []\n",
    "    possible_ssbond_id = []\n",
    "    for i in range(len(map_list)-1):\n",
    "        for j in range(i+1, len(map_list)):\n",
    "            if i == j: # should not happen\n",
    "                continue\n",
    "            elif mol_type_list[i][1:] == mol_type_list[j][1:]: # should not happen\n",
    "                continue\n",
    "            elif compare_CA_distance(map_list[i][1], map_list[j][1], mol_type_list[i], mol_type_list[j]):\n",
    "                temp = map_list[i][:]\n",
    "                temp.extend(map_list[j])\n",
    "                possible_ssbond.append(temp)\n",
    "                possible_ssbond_id.append((mol_type_list[i], mol_type_list[j]))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return possible_ssbond, possible_ssbond_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) noSG_restore_fnn.......................................................................................\n",
    "\n",
    "def set_pointdir(basepath):\n",
    "    checkpoint_dir = os.path.join(basepath, 'SSBONDPredict/PreDisulfideBond/static/newmodel')\n",
    "    return checkpoint_dir\n",
    "#add energy, only add a parameter result_E into predict\n",
    "#def predict(args, sess, images, labels, logits, out):\n",
    "def predict(args, sess, images, labels, logits, out):\n",
    "    data = args[0]\n",
    "    id_ord = args[1]\n",
    "    name = args[2]\n",
    "    out_ = sess.run(out, feed_dict={images:data.reshape((len(data), 100))}) # run the model, feed_dict is a dict precising what x and y placeholders are fed with\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    new_list=[]\n",
    "    new_list_score = []\n",
    "    for outi in range(len(out_)):\n",
    "        # calculating entropy\n",
    "        # if out_[outi][1] > out_[outi][0]:\n",
    "        #     count += 1\n",
    "        #     number1 = id_ord[outi][0][4:]\n",
    "        #     number2 = id_ord[outi][1][4:]\n",
    "        #     distance = abs( int(number1) - int(number2) )\n",
    "        #     if distance != 0:\n",
    "        #         t = math.log(distance, )\n",
    "        #         s = -2.1 - 1.5*8.314*t # volume\n",
    "        #         s = '%.4f'% s\n",
    "        #     else:\n",
    "        #         s = -2.1\n",
    "        #         s = '%.4f'% s\n",
    "            result_dict[id_ord[outi][0]+'-'+id_ord[outi][1]] = str('%.3f'% out_[outi][1]) #+ ' ' + str(s)\n",
    "    print('finish predict.')\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def main(args, basepath):\n",
    "    sess=tf.compat.v1.Session()\n",
    "    checkpoint_dir = set_pointdir(basepath)\n",
    "#    ckpt = tf.train.checkpoint_exists(checkpoint_dir)\n",
    "    ckpt_path = os.path.join(checkpoint_dir, 'model.ckpt-800')\n",
    "\n",
    "    saver = tf.compat.v1.train.import_meta_graph(ckpt_path + '.meta')\n",
    "    saver.restore(sess, ckpt_path)\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    images = graph.get_tensor_by_name('image:0')\n",
    "    labels = graph.get_tensor_by_name('labels:0')\n",
    "    logits = graph.get_tensor_by_name('softmax_linear/add:0')\n",
    "\n",
    "    out = tf.nn.softmax(logits=logits)\n",
    "\n",
    "    result_dict = predict(args, sess, images, labels, logits, out)\n",
    "    return result_dict\n",
    "\n",
    "## There is a contradiction here!!! they use an input of 100, while in the original paper, they say 45!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions of SSBondPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all predictions from SSBONDPREDICT\n",
    "pdb_files = sorted(glob.glob(\"data/pdbs/cleaned/*_clean.pdb\"))\n",
    "\n",
    "ssbond_df = []\n",
    "for pdb_file in pdb_files:\n",
    "    print(pdb_file)\n",
    "    pdb_id = os.path.basename(pdb_file.split(\"_clean\")[0])\n",
    "    PositionOfThisProject = \".\"\n",
    "    preds = process_pdb(args=pdb_file,\n",
    "                    PositionOfThisProject=PositionOfThisProject)\n",
    "    preds = pd.DataFrame(preds.items(),\n",
    "                         columns=[\"ssbond_format\", \"p_ss\"])\n",
    "    preds[\"pdb_id\"] = pdb_id\n",
    "    preds[\"ss\"] = 0\n",
    "    ssbond_df.append(preds)\n",
    "\n",
    "ssbond_df = pd.concat(ssbond_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssbonds = pd.read_pickle(\"data/tm_ssbonds_table\")\n",
    "\n",
    "with open(\"data/nb_mapper.pickle\", \"rb\") as f:\n",
    "    nb_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapper = {v: k for k, v in nb_mapper.items()}\n",
    "assert len(inv_mapper) == len(nb_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssbonds[\"pdb_id\"] = \"gcl1_apo_final_york_experimental_structure\"\n",
    "ssbonds[\"ss\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssbond_df.ssbond_format = ssbond_df.ssbond_format.str.split(\"-\")\n",
    "\n",
    "for i in range(2):\n",
    "    ssbond_df[f\"res{i+1}_id\"] = ssbond_df.ssbond_format.apply(\n",
    "        lambda x: [\n",
    "        x[i][:3],\n",
    "        int(x[i][4:]), # position\n",
    "        x[i][3] # chain\n",
    "          ])\n",
    "\n",
    "    ssbond_df[f\"res{i+1}\"] = ssbond_df[f\"res{i+1}_id\"].apply(\n",
    "        lambda x: \n",
    "            Bio.PDB.Polypeptide.three_to_index(x[0])\n",
    "    )\n",
    "\n",
    "ssbond_df.drop(columns=f\"ssbond_format\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/ssbondpredict_nb_mapper.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(ssbondpredict_mapper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssbond_df[\"ss\"]\n",
    "mask_indices = []\n",
    "for pdb_id, res1, res2 in zip(ssbonds.pdb_id, ssbonds.res1_id, ssbonds.res2_id):\n",
    "    try:\n",
    "        mask_indices.append(\n",
    "            ssbond_df[\n",
    "                (ssbond_df.res1_id.apply(lambda x: x[1]) == ssbondpredict_mapper[res1[1]]) & \n",
    "                (ssbond_df.res2_id.apply(lambda x: x[1]) == ssbondpredict_mapper[res2[1]])\n",
    "            ].index[0]\n",
    "        )\n",
    "\n",
    "    except (KeyError, IndexError):\n",
    "        print(f\"{res1[1]} or {res2[1]} not included.\")\n",
    "\n",
    "ssbond_df.ss.iloc[mask_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssbond_df.to_pickle(f\"data/preds_ssbond\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ROC and PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, f1_score, auc,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# model_name = \"m1_bigger_backbone_b1000_5atoms_matfact_1_lr_2\" # parsed_450\n",
    "# dset_name = \"parsed_450\"\n",
    "# model_name = \"m1_bigger_backbone_b1000_5atoms_matfact_1_lr_3_q_1\" # parsed_325 (no weights)\n",
    "dset_name = \"parsed_325\"\n",
    "model_name = \"m1_bigger_backbone_b1000_5atoms_matfact_1_lr_3_q_1_weighted\" # parsed_325 (weighted)\n",
    "\n",
    "# model_name = \"idp_325_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load cavity model's predictions\n",
    "preds_cav = pd.read_pickle(f\"data/preds_cav_{model_name}\")\n",
    "preds_cav[\"p_ss\"] = preds_cav.apply(lambda x: x.preds[21], axis=1)\n",
    "\n",
    "with open(\"data/nb_mapper.pickle\", \"rb\") as f:\n",
    "    nb_mapper = pickle.load(f)\n",
    "inv_mapper = {v: k for k, v in nb_mapper.items()}\n",
    "\n",
    "\n",
    "# 2) Load SSBondPredict's predictions\n",
    "preds_sspred = pd.read_pickle(f\"data/preds_ssbond\")\n",
    "preds_sspred.p_ss = preds_sspred.p_ss.astype(float)\n",
    "\n",
    "with open(\"data/ssbondpredict_nb_mapper.pickle\", \"rb\") as f:\n",
    "    sspredict_mapper = pickle.load(f)\n",
    "\n",
    "\n",
    "# 3) Load ssbonds table\n",
    "ssbonds = pd.read_pickle(f\"data/ssbonds_preds_cav_{model_name}\")\n",
    "\n",
    "# 4) For a fairer comparison, ignore pairs containing PRO and GLY\n",
    "preds_cav_same_dset = preds_cav[(~preds_cav.res1.isin([12, 5])) & (~preds_cav.res2.isin([12, 5]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pair_index(res1, res2):\n",
    "    three_to_index = Bio.PDB.Polypeptide.three_to_index\n",
    "    return np.ravel_multi_index(\n",
    "        [three_to_index(res1[0]), three_to_index(res2[0])],\n",
    "        (20, 20)\n",
    "        )\n",
    "\n",
    "def plot_roc_curve(cav_ss, cav_pss,\n",
    "                   ssbond_ss, ssbond_pss,\n",
    "                   save_name=\"\",\n",
    "    ):\n",
    "    cav_fpr, cav_tpr, cav_thresholds = roc_curve(y_true=cav_ss,\n",
    "                                                y_score=cav_pss, )\n",
    "    cav_auc = roc_auc_score(cav_ss, cav_pss)\n",
    "\n",
    "    ssbond_fpr, ssbond_tpr, ssbond_thresholds = roc_curve(y_true=ssbond_ss,\n",
    "                                                        y_score=ssbond_pss, )\n",
    "    ssbond_auc = roc_auc_score(y_true=ssbond_ss, y_score=ssbond_pss)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    plt.figure(figsize=(5, 4), constrained_layout=True)\n",
    "\n",
    "\n",
    "    plt.plot(cav_fpr, cav_tpr, marker='.', label=f'cav_model: auc = {cav_auc:5.3f}')\n",
    "    plt.plot(ssbond_fpr, ssbond_tpr, marker='.', label=f'ssbond: auc = {ssbond_auc:5.3f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', linewidth=1) #, label=\"random classifier\")\n",
    "\n",
    "    legend = plt.legend(shadow=True)\n",
    "    frame = legend.get_frame()\n",
    "    frame.set_facecolor('white')\n",
    "    frame.set_edgecolor('black')\n",
    "    plt.grid(linestyle=\"-\", alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"1 - Specificity (FPR)\")\n",
    "    plt.ylabel(\"Sensitivity (TPR)\")\n",
    "    if save_name != \"\":\n",
    "        plt.savefig(f\"results/benchmark/roc_ssbond_cav_{save_name}.png\",\n",
    "                        dpi=200, bbox_inches = \"tight\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(preds_cav_same_dset.ss, preds_cav_same_dset.p_ss,\n",
    "               preds_sspred.ss, preds_sspred.p_ss,\n",
    "               save_name=f\"same_dset_{model_name}\")\n",
    "\n",
    "plot_roc_curve(preds_cav.ss, preds_cav.p_ss,\n",
    "               preds_sspred.ss, preds_sspred.p_ss,\n",
    "               save_name=f\"diff_dset_{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recal curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In terms of model selection,\n",
    "# F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5),\n",
    "# whereas the area under curve summarize the skill of a model across thresholds, like ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(cav_ss, cav_pss,\n",
    "                   ssbond_ss, ssbond_pss,\n",
    "                   save_name=\"\",\n",
    "    ):\n",
    "    cav_precision, cav_recall, _ = precision_recall_curve(cav_ss,\n",
    "                                                          cav_pss, )\n",
    "    cav_auc = auc(cav_recall, cav_precision)\n",
    "\n",
    "    ssbond_precision, ssbond_recall, _ = precision_recall_curve(ssbond_ss,\n",
    "                                                                ssbond_pss, )\n",
    "    ssbond_auc = auc(ssbond_recall, ssbond_precision)\n",
    "\n",
    "    # plot the PR curve for the model\n",
    "    plt.figure(figsize=(5, 4), constrained_layout=True)\n",
    "\n",
    "\n",
    "    plt.plot(cav_recall, cav_precision, marker='.',\n",
    "            label=f'cav_model: auc = {cav_auc:5.4f}')\n",
    "    plt.plot(ssbond_recall, ssbond_precision, marker='.',\n",
    "            label=f'ssbond: auc = {ssbond_auc:5.4f}')\n",
    "\n",
    "    plt.axhline(y=sum(cav_ss == 1)/len(cav_ss),\n",
    "                linestyle='--',\n",
    "                linewidth=1)\n",
    "\n",
    "    legend = plt.legend(shadow=True)\n",
    "    frame = legend.get_frame()\n",
    "    frame.set_facecolor('white')\n",
    "    frame.set_edgecolor('black')\n",
    "    plt.grid(linestyle=\"-\", alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"log( Recall (TPR) )\")\n",
    "    plt.ylabel(\"log( Precision (PPV) )\")\n",
    "\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.savefig(f\"results/benchmark/pr_ssbond_cav_{save_name}.png\",\n",
    "                    dpi=200, bbox_inches = \"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pr_curve(preds_cav_same_dset.ss, preds_cav_same_dset.p_ss,\n",
    "               preds_sspred.ss, preds_sspred.p_ss,\n",
    "               save_name=f\"same_dset_{model_name}\")\n",
    "\n",
    "plot_pr_curve(preds_cav.ss, preds_cav.p_ss,\n",
    "               preds_sspred.ss, preds_sspred.p_ss,\n",
    "               save_name=f\"diff_dset_{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, probas_pred, threshold,\n",
    "                          model_name,\n",
    "                          save_name=\"model\",\n",
    "                          save=False,\n",
    "                          cmap=\"viridis\"):\n",
    "\n",
    "    sns.set(font_scale=1.2, style=\"ticks\")\n",
    "    cf_matrix = confusion_matrix(y_true, probas_pred > threshold)\n",
    "\n",
    "    group_names = [\"TN\",\"FP\",\"FN\",\"TP\"]\n",
    "    group_counts = [f\"{value}\" for value in cf_matrix.flatten()]\n",
    "    row_sums = cf_matrix.sum(axis=1)\n",
    "    norm_matrix = cf_matrix / row_sums[:, np.newaxis]\n",
    "    group_percentages = [f\"{value*100:.2f}%\" for value in norm_matrix.flatten()]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(norm_matrix, annot=labels, annot_kws={\"size\": 15}, \n",
    "                fmt=\"\", vmin=0, vmax=1, cmap=cmap, linewidths=1)\n",
    "    plt.ylabel(\"Predicted\", fontsize=13)\n",
    "    plt.xlabel(\"Predicted\", fontsize=13, labelpad=-0)\n",
    "\n",
    "    plt.title(f\"{model_name}, thr: {threshold}\", fontsize=15)\n",
    "    if save:\n",
    "        plt.savefig(f\"results/benchmark/cf_matrix_{save_name}.png\",\n",
    "                dpi=200, bbox_inches = \"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 9.5e-6\n",
    "plot_confusion_matrix(preds_cav_same_dset.ss, preds_cav_same_dset.p_ss, threshold,\n",
    "                      model_name=\"cav_model (common)\", save_name=f\"cav_model_same_{model_name}\", save=True)\n",
    "\n",
    "plot_confusion_matrix(preds_cav.ss, preds_cav.p_ss, threshold,\n",
    "                      model_name=\"cav_model (all)\", save_name=f\"cav_model_all_{model_name}\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.3e-2\n",
    "\n",
    "plot_confusion_matrix(preds_sspred.ss, preds_sspred.p_ss, threshold,\n",
    "                      model_name=\"cav_model (all)\", save_name=f\"ssbondpredict\", save=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNLZJ21sXq+NjJBHlEPfc9/",
   "name": "tm_corr_experiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cav_model",
   "language": "python",
   "name": "cav_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
